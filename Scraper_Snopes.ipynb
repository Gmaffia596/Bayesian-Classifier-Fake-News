{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper Snopes\n",
    "Scraper utilizzato per la creazione del dataset di fake news e di legit news. \n",
    "\n",
    "Esso prende dati:\n",
    "\n",
    "Dagli archivi di www.snopes.com per i collegamenti alle notizie false e per i testi delle notizie vere;\n",
    "\n",
    "Dalle pagine di archive.is per il testo degli articoli fals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import NavigableString\n",
    "from nltk import *\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articoli = []\n",
    "fake = []\n",
    "deception = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione lista degli archivi di Snopes per gli articoli falsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archive = open(\"snopes/archive.txt\",\"w\", encoding=\"utf8\")\n",
    "archive.write(\"http://www.snopes.com/category/facts/fake-news/\"+\"\\n\")\n",
    "articoli.append(\"http://www.snopes.com/category/facts/fake-news/\")\n",
    "i = 1\n",
    "while(i < 61):\n",
    "    archive_ = \"http://www.snopes.com/category/facts/fake-news/page/\"+str(i)+\"/\"\n",
    "    archive.write(archive_+\"\\n\")\n",
    "    articoli.append(archive_)\n",
    "    i = i+1\n",
    "archive.close()\n",
    "print(\"Creato in snopes/archive.txt l'archivo del sito, nel quale trovare tutti i collegamenti alle fake news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scraper(indirizzo,file):\n",
    "        addr = (requests.get(indirizzo).text)\n",
    "        zuppa = bs(addr,\"html.parser\").find_all({'article':True,'class' : True})\n",
    "        for text in zuppa:\n",
    "            for link in text.findAll('a',href=True):\n",
    "                file.write(link['href']+\"\\n\")\n",
    "                fake.append(link['href'])\n",
    "                \n",
    "def faker(indirizzo,file):\n",
    "        addr = (requests.get(indirizzo).text)\n",
    "        zuppa = bs(addr,\"html.parser\").find_all({'article':True,'p' : True})\n",
    "        for text in zuppa:\n",
    "            for link in text.findAll('a',href=True):\n",
    "                if(\"archive.is\" in link['href']):\n",
    "                    file.write(link['href']+\"\\n\")\n",
    "                    deception.append(link['href'])\n",
    "                    \n",
    "def journalist(indirizzo,file):\n",
    "        raw = []\n",
    "        rawr = \"\"\n",
    "        addr = (requests.get(indirizzo).text)\n",
    "        zuppa = bs(addr,\"html.parser\").findAll('article')#(div, style=\"text-align:left;font-size:100%;vertical-align:baseline;display:block;border-width: 0px; border-style: none; margin: 0px 0px 20px; padding: 0px; border-color: white; \")\n",
    "        for a in zuppa:\n",
    "            for b in a.findAll('div'):\n",
    "                for c in b.findAll(text=True):\n",
    "                    raw.append(c.replace(\"\\n\",\"\")+\" \")\n",
    "        zuppa = bs(addr,\"html.parser\").findAll('blockquote')\n",
    "        for a in zuppa:\n",
    "            raw.append(str(a.getText()))\n",
    "        for result in raw:\n",
    "            rawr = rawr + result \n",
    "        file.write(rawr)\n",
    "            \n",
    "print(\"definite le classi per scaricare i dati dal web\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione lista articoli falsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "articles = open(\"snopes/articles.txt\",\"w\", encoding=\"utf8\")\n",
    "for data in articoli:\n",
    "    scraper(data,articles)\n",
    "articles.close()\n",
    "print(\"scaricati i siti di snopes che facevano riferimento a fake news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione link ad Archive.is per il raggiungimento del testo degli articoli falsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fakes = open(\"snopes/fake.txt\",\"w\",encoding=\"utf8\")\n",
    "for data in fake:\n",
    "    faker(data,fakes)\n",
    "fakes.close()\n",
    "print(\"scaricati i siti relativi alle fake news originali\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controllo sugli articoli per eventuali errori in download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verified_ = open(\"snopes/verified_.txt\",\"w\",encoding=\"utf8\")\n",
    "verified = []\n",
    "for data in deception:\n",
    "    if(data not in verified):\n",
    "        if(\"image\" not in data):\n",
    "            verified.append(data)\n",
    "            verified_.write(data+\"\\n\")\n",
    "print(\"verifica dei link\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download testo degli articoli falsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for data in verified:\n",
    "    journalists = open(\"snopes/dataset/fake/\"+str(i)+\"_fake.txt\",\"w\",encoding=\"utf8\")\n",
    "    journalist(data,journalists)\n",
    "    journalists.close()\n",
    "    i = i+1\n",
    "print(\"scaricato il testo degli articoli delle fake news \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articoli_ = []\n",
    "legit = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione lista archivio per articoli veri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archive_ = open(\"snopes/archive_.txt\",\"w\", encoding=\"utf8\")\n",
    "archive_.write(\"http://www.snopes.com/news\"+\"\\n\")\n",
    "articoli_.append(\"http://www.snopes.com/news\")\n",
    "i = 1\n",
    "while(i < 193):\n",
    "    raw = \"http://www.snopes.com/news/page/\"+str(i)+\"/\"\n",
    "    archive_.write(raw+\"\\n\")\n",
    "    articoli_.append(raw)\n",
    "    i = i+1\n",
    "    \n",
    "print(\"Creato in snopes/archive_.txt l'archivo del sito, nel quale trovare tutti i collegamenti alle notizie vere\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scraper_(indirizzo,file):\n",
    "        addr = (requests.get(indirizzo).text)\n",
    "        zuppa = bs(addr,\"html.parser\").find_all({'article':True,'class' : True})\n",
    "        for text in zuppa:\n",
    "            for link in text.findAll('a',href=True):\n",
    "                file.write(link['href']+\"\\n\")\n",
    "                legit.append(link['href'])\n",
    "                \n",
    "def legit_ (indirizzo,file):\n",
    "    addr=(requests.get(indirizzo).text)\n",
    "    zuppa = bs(addr,\"html.parser\").findAll('div')\n",
    "    for data in zuppa:\n",
    "        for text in data.findAll(attrs={\"class\" : \"entry-content article-text legacy\"}):\n",
    "             for article in text.findAll(\"p\"):\n",
    "                    if(\"Got a tip or a rumor?\" in article.getText()):\n",
    "                        break;\n",
    "                    else:\n",
    "                        file.write(article.getText())\n",
    "print(\"definite le classi per scaricare i dati dal web\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creazione lista articoli veri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "articles_ = open(\"snopes/articles_.txt\",\"w\", encoding=\"utf8\")\n",
    "for data in articoli_:\n",
    "    scraper_(data,articles_)\n",
    "articles_.close()\n",
    "print(\"scaricati i siti dagli archivi snopes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download testo articoli veri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for data in legit:\n",
    "    legits = open(\"snopes/dataset/legit/\"+str(i)+\"_legit.txt\",\"w\", encoding=\"utf8\")\n",
    "    legit_(data, legits)\n",
    "    legits.close()\n",
    "    i = i+1\n",
    "print(\"scaricati gli articoli verificati\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Totale articoli scaricati: 1127"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
